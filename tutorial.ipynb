{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alrao (All learning rates at once) : a tutorial\n",
    "\n",
    "We show in this notebook how to use Alrao in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from alrao import AlraoModel\n",
    "from alrao import SGDAlrao, AdamAlrao\n",
    "from alrao import lr_sampler_generic, generator_randomlr_neurons, generator_randomlr_weights\n",
    "\n",
    "# CUDA\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "We use the CIFAR10 dataset. We also use some data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the preclassifier model\n",
    "We define a pre-classifier model. This model can be defined exactly as any usual model. Only two things are specific with alrao : \n",
    "* First, there is no classifier. The classifier layer will be added later\n",
    "* The model needs the have a `linearinputdim` attribute , which is the output's dimension of the pre-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module): # identical to models.VGG\n",
    "    def __init__(self, cfg):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg)\n",
    "        # The dimension of the preclassier's output need to be specified.\n",
    "        self.linearinputdim = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # The model do not contain a classifier layer.\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "preclassifier = VGG([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M',\n",
    "                     512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classifier\n",
    "Here, we define our own classifier class. In practice, we do not need to redefine it, it can be found in `alrao.custom_layers.LinearClassifier`.\n",
    "We redefine it here to show how any classifier (with a log-softmax output) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module): # identical to alrao.custom_layers.LinearClassifier\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the modified architecture for Alrao\n",
    "We define the new architecture, with the parallel classifiers.\n",
    "<img src=\"img/newalrao.png\" width=\"400\"></img>\n",
    "\n",
    "Here there are 10 categories, and we decide to use 10 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_classifiers is the number of classifiers averaged by Alrao.\n",
    "nb_classifiers = 10\n",
    "nb_categories = 10\n",
    "net = AlraoModel(preclassifier, nb_classifiers, Classifier, preclassifier.linearinputdim, nb_categories)\n",
    "if use_cuda: net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the learning rates\n",
    "We choose an interval (`minLR`, `maxLR`) in which the learning rates are chosen.\n",
    "For the pre-classifier, the learning rates are sampled from the log-uniform distribution $\\log-U(\\cdot ; \\eta_{\\min}, \\eta_{\\max})$ :\n",
    "namely, if $\\eta \\sim \\log-U(\\cdot ; \\eta_{\\min},\n",
    "\\eta_{\\max})$, then $\\log \\eta$ is uniformly distributed between $\\log\n",
    "\\eta_{\\min}$ and $\\log \\eta_{\\max}$.\n",
    "Its\n",
    "density function is\n",
    "$$\\log-U(\\eta; \\eta_\\min, \\eta_\\max) = \\frac{1_{\\eta_\\min \\leq \\eta \\leq \\eta_\\max}}{\\eta_\\max - \\eta_\\min}\\times\\frac{1}{\\eta}$$\n",
    "\n",
    "The learning rates of the classifier are log-uniformly spread on the interval : \n",
    "$\\log \\eta_{j} = \\log \\eta_{\\min} +\n",
    "\\frac{j-1}{N_{\\mathrm{cl}}-1}\\log(\\eta_{\\max}/ \\eta_{\\min})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the interval in which the learning rates are sampled\n",
    "minlr = 10 ** (-5)\n",
    "maxlr = 10 ** 1\n",
    "\n",
    "# We spread the classifiers learning rates log-uniformly on the interval.\n",
    "classifiers_lr = [np.exp(np.log(minlr) + \\\n",
    "    k /(nb_classifiers-1) * (np.log(maxlr) - np.log(minlr)) \\\n",
    "    ) for k in range(nb_classifiers)]\n",
    "\n",
    "# We define the sampler for the preclassifier’s features.\n",
    "lr_sampler = lr_sampler_generic(minlr, maxlr)\n",
    "lr_preclassifier = generator_randomlr_neurons(net.preclassifier, lr_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the optimizer\n",
    "We define the Alrao optimizer. This includes : \n",
    "* A single (usual) SGD optimizer for each classifier\n",
    "* A modified SGD optimizer for the pre-classifier, allowing to use one learning rate per neuron.\n",
    "* The switch model averaging method, with its own update procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGDAlrao(net.parameters_preclassifier(),\n",
    "                     lr_preclassifier,\n",
    "                     net.classifiers_parameters_list(),\n",
    "                     classifiers_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "We define the train procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(total=len(trainloader.dataset),bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} {postfix}')\n",
    "    pbar.set_description(\"Epoch %d\" % epoch)\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        net.train()\n",
    "        if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # We update the model averaging weights in the optimizer\n",
    "        optimizer.update_posterior(net.posterior())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass of the Alrao model\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # We compute the gradient of all the model’s weights\n",
    "        loss.backward()\n",
    "\n",
    "        # We reset all the classifiers gradients, and re-compute them with\n",
    "        # as if their were the only output of the network.\n",
    "        optimizer.classifiers_zero_grad()\n",
    "        newx = net.last_x.detach()\n",
    "        for classifier in net.classifiers():\n",
    "            loss_classifier = criterion(classifier(newx), targets)\n",
    "            loss_classifier.backward()\n",
    "\n",
    "        # Then, we can run an update step of the gradient descent.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Finally, we update the model averaging weights\n",
    "        net.update_switch(targets, catch_up=False)\n",
    "\n",
    "        # Update loss\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progression bar\n",
    "        pbar.update(batch_size)\n",
    "        postfix = OrderedDict([(\"LossTrain\",\"{:.4f}\".format(train_loss/(batch_idx+1))),\n",
    "                               (\"AccTrain\", \"{:.3f}\".format(100.*correct/total))])\n",
    "        postfix[\"PostSw\"] = net.repr_posterior()\n",
    "        pbar.set_postfix(postfix)\n",
    "    pbar.close()\n",
    "\n",
    "    # Print performance of the classifiers\n",
    "    cl_perf = net.switch.get_cl_perf()\n",
    "    for k in range(len(cl_perf)):\n",
    "        print(\"Classifier {}\\t LossTrain:{:.6f}\\tAccTrain:{:.4f}\".format(\n",
    "            k, cl_perf[k][0], cl_perf[k][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    net.eval()\n",
    "    net.switch.reset_cl_perf()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        net.eval()\n",
    "        if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # Forward pass of the Alrao model\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update loss\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\tLossTest: %.4f\\tAccTest: %.3f' % (test_loss/(batch_idx+1), 100.*correct/total))\n",
    "    print((\"Posterior : \"+\"{:.1e}, \" * nb_classifiers).format(*net.posterior()))\n",
    "\n",
    "    return test_loss / (batch_idx + 1), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/50000 /usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Epoch 0: : 50016it [01:07, 742.92it/s, LossTrain=1.4900, AccTrain=45.114, PostSw=|   █      |]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0\t LossTrain:2.173622\tAccTrain:0.1930\n",
      "Classifier 1\t LossTrain:2.163998\tAccTrain:0.2271\n",
      "Classifier 2\t LossTrain:1.804787\tAccTrain:0.3675\n",
      "Classifier 3\t LossTrain:1.492775\tAccTrain:0.4513\n",
      "Classifier 4\t LossTrain:1.494896\tAccTrain:0.4512\n",
      "Classifier 5\t LossTrain:1.563066\tAccTrain:0.4236\n",
      "Classifier 6\t LossTrain:2.965732\tAccTrain:0.3495\n",
      "Classifier 7\t LossTrain:15.103052\tAccTrain:0.3258\n",
      "Classifier 8\t LossTrain:71.228097\tAccTrain:0.3261\n",
      "Classifier 9\t LossTrain:336.374353\tAccTrain:0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%|          | 0/50000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLossTest: 1.9365\tAccTest: 40.080\n",
      "Posterior : 1.0e-04, 1.3e-04, 3.1e-04, 9.5e-01, 5.1e-02, 1.0e-03, 9.2e-05, 6.3e-05, 6.3e-05, 6.3e-05, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 50016it [01:07, 736.07it/s, LossTrain=1.0356, AccTrain=63.354, PostSw=|   █      |]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0\t LossTrain:2.044145\tAccTrain:0.2570\n",
      "Classifier 1\t LossTrain:1.714333\tAccTrain:0.5242\n",
      "Classifier 2\t LossTrain:1.236024\tAccTrain:0.6049\n",
      "Classifier 3\t LossTrain:1.035507\tAccTrain:0.6336\n",
      "Classifier 4\t LossTrain:1.046654\tAccTrain:0.6290\n",
      "Classifier 5\t LossTrain:1.099860\tAccTrain:0.6112\n",
      "Classifier 6\t LossTrain:1.919717\tAccTrain:0.5359\n",
      "Classifier 7\t LossTrain:8.849318\tAccTrain:0.5079\n",
      "Classifier 8\t LossTrain:41.872616\tAccTrain:0.5052\n",
      "Classifier 9\t LossTrain:193.838003\tAccTrain:0.5055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%|          | 0/50000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLossTest: 0.9920\tAccTest: 65.720\n",
      "Posterior : 4.3e-05, 5.7e-05, 2.0e-04, 1.0e+00, 2.5e-03, 5.4e-04, 9.2e-05, 5.1e-05, 3.1e-05, 3.1e-05, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 50016it [01:07, 741.45it/s, LossTrain=0.8494, AccTrain=70.306, PostSw=|   █      |]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0\t LossTrain:1.879200\tAccTrain:0.3608\n",
      "Classifier 1\t LossTrain:1.381986\tAccTrain:0.6621\n",
      "Classifier 2\t LossTrain:0.972673\tAccTrain:0.6900\n",
      "Classifier 3\t LossTrain:0.849395\tAccTrain:0.7031\n",
      "Classifier 4\t LossTrain:0.861581\tAccTrain:0.6990\n",
      "Classifier 5\t LossTrain:0.912557\tAccTrain:0.6842\n",
      "Classifier 6\t LossTrain:1.632264\tAccTrain:0.6169\n",
      "Classifier 7\t LossTrain:7.417791\tAccTrain:0.5905\n",
      "Classifier 8\t LossTrain:35.361829\tAccTrain:0.5900\n",
      "Classifier 9\t LossTrain:164.323780\tAccTrain:0.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:   0%|          | 0/50000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLossTest: 1.0819\tAccTest: 64.330\n",
      "Posterior : 4.2e-05, 7.9e-05, 3.5e-04, 1.0e+00, 1.8e-03, 2.8e-04, 2.2e-05, 2.0e-05, 2.0e-05, 2.0e-05, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 50016it [01:07, 744.65it/s, LossTrain=0.7438, AccTrain=74.348, PostSw=|   █      |]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0\t LossTrain:1.697140\tAccTrain:0.5044\n",
      "Classifier 1\t LossTrain:1.150226\tAccTrain:0.7131\n",
      "Classifier 2\t LossTrain:0.820588\tAccTrain:0.7345\n",
      "Classifier 3\t LossTrain:0.743760\tAccTrain:0.7434\n",
      "Classifier 4\t LossTrain:0.755788\tAccTrain:0.7387\n",
      "Classifier 5\t LossTrain:0.803319\tAccTrain:0.7257\n",
      "Classifier 6\t LossTrain:1.449954\tAccTrain:0.6628\n",
      "Classifier 7\t LossTrain:6.610083\tAccTrain:0.6379\n",
      "Classifier 8\t LossTrain:31.033267\tAccTrain:0.6384\n",
      "Classifier 9\t LossTrain:145.715637\tAccTrain:0.6370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:   0%|          | 0/50000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLossTest: 0.9804\tAccTest: 67.940\n",
      "Posterior : 2.3e-05, 4.6e-05, 2.5e-04, 1.0e+00, 1.4e-03, 4.0e-04, 7.2e-05, 2.1e-05, 1.5e-05, 1.5e-05, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 50016it [01:07, 741.40it/s, LossTrain=0.6640, AccTrain=77.040, PostSw=|   █      |]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0\t LossTrain:1.571790\tAccTrain:0.6018\n",
      "Classifier 1\t LossTrain:0.979954\tAccTrain:0.7528\n",
      "Classifier 2\t LossTrain:0.716525\tAccTrain:0.7655\n",
      "Classifier 3\t LossTrain:0.664020\tAccTrain:0.7704\n",
      "Classifier 4\t LossTrain:0.674792\tAccTrain:0.7672\n",
      "Classifier 5\t LossTrain:0.720209\tAccTrain:0.7528\n",
      "Classifier 6\t LossTrain:1.314522\tAccTrain:0.6977\n",
      "Classifier 7\t LossTrain:6.121748\tAccTrain:0.6753\n",
      "Classifier 8\t LossTrain:28.275020\tAccTrain:0.6742\n",
      "Classifier 9\t LossTrain:131.609205\tAccTrain:0.6744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLossTest: 0.8283\tAccTest: 72.440\n",
      "Posterior : 2.1e-05, 4.7e-05, 2.8e-04, 1.0e+00, 1.2e-03, 2.7e-04, 4.3e-05, 1.2e-05, 1.2e-05, 1.2e-05, \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
